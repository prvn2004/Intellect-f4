{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CREATE SEPERATE 10K IMAGE TRAIN DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CREATE JSON FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "\n",
    "def csv_to_json(csv_file, json_file):\n",
    "    # List to hold the final JSON data\n",
    "    json_data = []\n",
    "\n",
    "    # Open and read the CSV file\n",
    "    with open(csv_file, newline='', encoding='utf-8') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        \n",
    "        # Loop through each row in the CSV file\n",
    "        for row in reader:\n",
    "            index = row['index']\n",
    "            image_path = row['image_path']\n",
    "            user_input = row['user_input']\n",
    "            # output = row['output']\n",
    "\n",
    "            # Create the structure as per the format provided\n",
    "            message = {\n",
    "                \"messages\": [\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": [\n",
    "                            {\n",
    "                                \"type\": \"image\",\n",
    "                                \"image\": image_path\n",
    "                            },\n",
    "                            {\n",
    "                                \"type\": \"text\",\n",
    "                                \"text\": user_input\n",
    "                            }\n",
    "                        ]\n",
    "                    }\n",
    "                    # ,\n",
    "                    # {\n",
    "                    #     \"role\": \"assistant\",\n",
    "                    #     \"content\": [\n",
    "                    #         {\n",
    "                    #             \"type\": \"text\",\n",
    "                    #             \"text\": output\n",
    "                    #         }\n",
    "                    #     ]\n",
    "                    # }\n",
    "                ]\n",
    "            }\n",
    "            \n",
    "            # Append to the JSON data list\n",
    "            json_data.append(message)\n",
    "\n",
    "    # Write the data to a JSON file\n",
    "    with open(json_file, 'w', encoding='utf-8') as jsonfile:\n",
    "        json.dump(json_data, jsonfile, ensure_ascii=False, indent=4)\n",
    "\n",
    "# File paths for input CSV and output JSON\n",
    "csv_file_path = 'test_data.csv'\n",
    "json_file_path = 'output_test.json'\n",
    "\n",
    "# Call the conversion function\n",
    "csv_to_json(csv_file_path, json_file_path)\n",
    "\n",
    "print(f\"JSON data has been written to {json_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install accelerate==0.34.0 \\\n",
    "attrs==24.2.0 \\\n",
    "av==13.0.0 \\\n",
    "certifi==2024.8.30 \\\n",
    "charset-normalizer==3.3.2 \\\n",
    "einops==0.8.0 \\\n",
    "fancycompleter==0.9.1 \\\n",
    "filelock==3.15.4 \\\n",
    "flash-attn==2.6.3 \\\n",
    "fsspec==2024.9.0 \\\n",
    "huggingface-hub==0.24.6 \\\n",
    "idna==3.8 \\\n",
    "Jinja2==3.1.4 \\\n",
    "MarkupSafe==2.1.5 \\\n",
    "mpmath==1.3.0 \\\n",
    "networkx==3.3 \\\n",
    "numpy==2.1.1 \\\n",
    "nvidia-cublas-cu12==12.1.3.1 \\\n",
    "nvidia-cuda-cupti-cu12==12.1.105 \\\n",
    "nvidia-cuda-nvrtc-cu12==12.1.105 \\\n",
    "nvidia-cuda-runtime-cu12==12.1.105 \\\n",
    "nvidia-cudnn-cu12==9.1.0.70 \\\n",
    "nvidia-cufft-cu12==11.0.2.54 \\\n",
    "nvidia-curand-cu12==10.3.2.106 \\\n",
    "nvidia-cusolver-cu12==11.4.5.107 \\\n",
    "nvidia-cusparse-cu12==12.1.0.106 \\\n",
    "nvidia-nccl-cu12==2.20.5 \\\n",
    "nvidia-nvjitlink-cu12==12.6.68 \\\n",
    "nvidia-nvtx-cu12==12.1.105 \\\n",
    "packaging==24.1 \\\n",
    "pdbpp==0.10.3 \\\n",
    "pillow==10.4.0 \\\n",
    "psutil==6.0.0 \\\n",
    "Pygments==2.18.0 \\\n",
    "pyrepl==0.9.0 \\\n",
    "PyYAML==6.0.2 \\\n",
    "regex==2024.7.24 \\\n",
    "requests==2.32.3 \\\n",
    "safetensors==0.4.5 \\\n",
    "socksio==1.0.0 \\\n",
    "sympy==1.13.2 \\\n",
    "tokenizers==0.19.1 \\\n",
    "torch==2.4.1 \\\n",
    "torchvision==0.19.1 \\\n",
    "tqdm==4.66.5 \\\n",
    "triton==3.0.0 \\\n",
    "typing_extensions==4.12.2 \\\n",
    "urllib3==2.2.2 \\\n",
    "wmctrl==0.5 \\\n",
    "git+https://github.com/huggingface/transformers@98f5463fee0009538eb1ca9f7e466c6145e3a731\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install git+https://github.com/huggingface/transformers@21fac7abba2a37fae86106f87fcf9974fd1e3830 accelerate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import datetime\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
    "from torch.utils.data import Dataset\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from functools import partial\n",
    "from util.vision_util import process_vision_info\n",
    "from util.logutil import init_logger, get_logger\n",
    "\n",
    "output_dir = f'./train_output/{datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")}/'\n",
    "init_logger(output_dir)\n",
    "logger = get_logger()\n",
    "\n",
    "device = \"cuda:0\"\n",
    "\n",
    "class ToyDataSet(Dataset):\n",
    "    def __init__(self, data_path):\n",
    "        super().__init__()\n",
    "        with open(data_path, \"r\") as f:\n",
    "            self.data = json.load(f)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        images = []\n",
    "        try:\n",
    "            # Extract image paths from the messages\n",
    "            for message in item.get('messages', []):\n",
    "                if message.get('role') == 'user':\n",
    "                    for content in message.get('content', []):\n",
    "                        if content.get('type') == 'image':\n",
    "                            image_path = content.get('image')\n",
    "                            if image_path and os.path.isfile(image_path):\n",
    "                                images.append(image_path)\n",
    "                            else:\n",
    "                                logger.warning(f\"Image file {image_path} not found. Skipping this item.\")\n",
    "                                return None\n",
    "        except KeyError as e:\n",
    "            logger.error(f\"Missing key {e} in data item at index {idx}. Skipping this item.\")\n",
    "            return None\n",
    "        \n",
    "        item['images'] = images\n",
    "        return item\n",
    "\n",
    "def find_assistant_content_sublist_indexes(l):\n",
    "    start_indexes = []\n",
    "    end_indexes = []\n",
    "\n",
    "    for i in range(len(l) - 1):\n",
    "        if l[i] == 151644 and l[i + 1] == 77091:\n",
    "            start_indexes.append(i)\n",
    "            for j in range(i + 2, len(l)):\n",
    "                if l[j] == 151645:\n",
    "                    end_indexes.append(j)\n",
    "                    break\n",
    "\n",
    "    return list(zip(start_indexes, end_indexes))\n",
    "\n",
    "def collate_fn(batch, processor, device):\n",
    "    batch = [item for item in batch if item is not None]  # Remove None items\n",
    "\n",
    "    if not batch:\n",
    "        logger.warning(\"Empty batch encountered after filtering out None items. Skipping this batch.\")\n",
    "        return None, None\n",
    "\n",
    "    try:\n",
    "        messages = [m['messages'] for m in batch]\n",
    "        texts = [processor.apply_chat_template(msg, tokenize=False, add_generation_prompt=False) for msg in messages]\n",
    "        \n",
    "        # Process images and videos\n",
    "        image_paths = [item['images'] for item in batch]\n",
    "        image_inputs = []\n",
    "        video_inputs = []  # Assuming video_inputs are not used in this context\n",
    "\n",
    "        for paths in image_paths:\n",
    "            images = []\n",
    "            for path in paths:\n",
    "                try:\n",
    "                    # Load and preprocess image\n",
    "                    image = Image.open(path).convert(\"RGB\")\n",
    "                    images.append(image)\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error loading image {path}: {e}\")\n",
    "            image_inputs.append(images)\n",
    "        \n",
    "        # Process vision info\n",
    "        try:\n",
    "            image_inputs, video_inputs = process_vision_info(messages)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing vision info: {e}\")\n",
    "            return None, None\n",
    "\n",
    "        inputs = processor(\n",
    "            text=texts,\n",
    "            images=image_inputs,\n",
    "            videos=video_inputs,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "\n",
    "        input_ids_lists = inputs['input_ids'].tolist()\n",
    "        assert len(messages) == len(input_ids_lists)\n",
    "\n",
    "        labels_list = []\n",
    "        for ids_list in input_ids_lists:\n",
    "            label_ids = [-100] * len(ids_list)\n",
    "            for begin_end_indexs in find_assistant_content_sublist_indexes(ids_list):\n",
    "                label_ids[begin_end_indexs[0]+2:begin_end_indexs[1]+1] = ids_list[begin_end_indexs[0]+2:begin_end_indexs[1]+1]\n",
    "            labels_list.append(label_ids)\n",
    "\n",
    "        labels_ids = torch.tensor(labels_list, dtype=torch.int64)\n",
    "\n",
    "        return inputs, labels_ids\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing batch: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def write_chat_template(processor, output_dir):\n",
    "    output_chat_template_file = os.path.join(output_dir, \"chat_template.json\")\n",
    "    chat_template_json_string = json.dumps({\"chat_template\": processor.chat_template}, indent=2, sort_keys=True) + \"\\n\"\n",
    "    with open(output_chat_template_file, \"w\", encoding=\"utf-8\") as writer:\n",
    "        writer.write(chat_template_json_string)\n",
    "        logger.info(f\"Chat template saved in {output_chat_template_file}\")\n",
    "\n",
    "def train():\n",
    "    model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "        \"Qwen/Qwen2-VL-2B-Instruct\", torch_dtype=torch.bfloat16, attn_implementation=\"flash_attention_2\", device_map=\"auto\"\n",
    "    )\n",
    "\n",
    "    processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\", min_pixels=256*28*28, max_pixels=512*28*28, padding_side=\"right\")\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        ToyDataSet(\"output.json\"),\n",
    "        batch_size=3,\n",
    "        collate_fn=partial(collate_fn, processor=processor, device=device),\n",
    "        drop_last=True  # Ensure that incomplete batches are dropped\n",
    "    )\n",
    "\n",
    "    model.train()\n",
    "    epochs = 2\n",
    "    optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "    NUM_ACCUMULATION_STEPS = 2\n",
    "    for epoch in range(epochs):\n",
    "        accumulated_avg_loss = 0\n",
    "        steps = 0\n",
    "        for batch in train_loader:\n",
    "            if batch is None:\n",
    "                continue  # Skip processing if batch is None\n",
    "            \n",
    "            try:\n",
    "                inputs, labels = batch\n",
    "                if inputs is None or labels is None:\n",
    "                    logger.warning(\"Skipping batch due to None inputs or labels.\")\n",
    "                    continue\n",
    "                \n",
    "                outputs = model(**inputs, labels=labels)\n",
    "                \n",
    "                loss = outputs.loss / NUM_ACCUMULATION_STEPS\n",
    "                accumulated_avg_loss += loss.item()\n",
    "                loss.backward()\n",
    "                \n",
    "                if steps % NUM_ACCUMULATION_STEPS == 0:\n",
    "                    logger.info(f\"Batch {steps} of epoch {epoch + 1}/{epochs}, average training loss of previous {NUM_ACCUMULATION_STEPS} batches: {accumulated_avg_loss}\")\n",
    "                    accumulated_avg_loss = 0\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                # Free GPU memory\n",
    "                torch.cuda.empty_cache()\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing batch {steps}: {e}\")\n",
    "\n",
    "            steps += 1\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    model.save_pretrained(output_dir)\n",
    "    processor.save_pretrained(output_dir)\n",
    "    write_chat_template(processor, output_dir)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "def zip_folder(folder_name, output_name):\n",
    "    with zipfile.ZipFile(output_name, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        for root, dirs, files in os.walk(folder_name):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                arcname = os.path.relpath(file_path, folder_name)\n",
    "                zipf.write(file_path, arcname)\n",
    "\n",
    "zip_folder('./train_output/20240915191425', 'train_output.zip')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
